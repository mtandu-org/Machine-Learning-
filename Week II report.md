## Week II report ParrotAI IPT

followed by little disscussion leading by Fredy the session was interesting from that discussion i have got that the only major 
different between ML and deep learning was in ML we have input data then data enginers need to extract the best features to attain 
better perfomance while in Deep Learnng we have an input data both classfication and features extraction done by model itself to 
atain best perfomance.   
beside that there is a lot of field which Deep learning perfom well include
 * Image captioning,Object Detection,
 * Image style Transfer,Self driving cars,
 * Machine translation ,Dignosys of Different Diseases like Pnemonia using Chest Xray images.

 You may ask yourself why now? There were no expart back in time the answer will be no since The biggest acomplishment of AI 
and Deep Learning are acompaned by the following
* availability of Big data,
* Increasing in  computational power by having alot of clouds computing device like google colab,Azure and so on
* improving technology and major one is having open source tools and Modules like Tensorflow,Pytoch and models which are trained   better in large data like Alexnet,VG,ImageNet and many more.
 
  Apart from that I learned about this stunning Technique to help your network /model perfom well in unseen data known as Regulization which consist of Dropout,Ealy stopping and Data Augumantation in short i can say a litle about these
 * Dropout is the technique in which the learning network remove random hidden layers during training step based on the probability and with low weights,
* Ealy stoppying is the idea based on stopping your model iteration when it start to overfit.
* Data augumentation   based on adding some  features on your data this can be atained by either of reduce the intensity of your data,forexample in image we can either croping ,flip ,Traslate ,rotate and many more.

 More sore i extract the knowledge from video tutorial about Deep Learning software and hardware where I was get ideas about GPU where this are the graphics processing unit which are used to rendering the graphics its good for parallel tasks since it has 1000 cores compared to cpu which maximum cores are ten.  
    Beside that Ilearned about different between different framework like Tensorflow,Caffe,Pytorch and Torch which all them are      best in different case forexample pytorch are best for research while Caffe2 is for deployment and application from here i was    stated to dig deep in pytorch practical implemantation
  
   Also I learned about Transfer Learning, Convulational neural Network(CNN) the best network for computer vision in both theory part  and practical part by implementing Lab3 and Lab4 back to Transfer Learning,In rel problem we suffer insufficient of data which can lead worse model based on your problem,here is  where transfer Learning arise . we train ur model in pretrained state of art which were trained in many images forexample AlexNet in more than 1.2 millions with 1000 different category by changing output layer acoding to your problem
   
  Lastly, We had a better session from Mr Antony concerning  Recurrent Neural Network this model are work much better in sequence data which posses
* varible lenth in both input and output
* keep tracking of dependences 
* parameter sharing 
  forample time series ,object tracking  in video,music generation ,speech recognition and many more
.The types of RNN which are LSTM and GRU. The technique succesfull in different scenario like Sentimental Analysis, Language Modeling,Text Generation,Speech recognition,Music Generation,Image captioning and time series data forecasting forexample Final Forecasting,wether forecasting and so on 
